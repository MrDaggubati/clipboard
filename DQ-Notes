

Accuracy	
    Domain membership rules specify that an attribute's value must be taken from a defined value domain (or reference table)
	A state data element must take its value from the list of United States Postal Service (USPS) 2-character state codes
Accuracy	
    The data attribute's value must be within a defined data range	
	The score must be between 0 and 100
Structural consistency	
    The data attribute's value must conform to a specific data type, length, and pattern	
	A tax_identifier data element must conform to the pattern 99-9999999
Completeness	
    Completeness rules specify whether a data field may or may contain null values	
	The product_code field may not be null
Currency	
    The data element's value has been refreshed within the specified time period	
	The product_price field must be refreshed at least once every 24 hours
Reasonableness	
    The data element's value must conform to reasonable expectations	
	The driver_age field may not be less than 16

	
	
Cross-Column/Record Dependency Rules

Dimension

Accuracy	
A data element's value is accurate relative to a system of record when the value is dependent on other data element 
values for system of record lookup	
Verify that the last_name field matches the system of record associated with the customer_identifier field

Accuracy	
A data element's value is taken from a subset of a defined value domain based on other data attributes' values	
Validate that the purchaser_code is valid for staff members based on cost_center

Consistency	
One data element's value is consistent with other data elements' 
values	The end_date must be later than the start_date

Completeness	When other data element values observe a defined condition, a data element's value is not null	
If security_product_type is “option” then the underlier field must not be null

Reasonableness	When other data element values observe a defined condition, a data element's value must conform to reasonable expectations	
Purchase_total must be less than credit_limit
Currency	When other data element values observe a defined condition, a data element's value has been refreshed within the specified time period	
If last_payment_date is after the last_payment_due, then refresh the finance_charge

Transformation	A data element's value is computed as a function of one or more other data attribute 
values	Line_item_total is calculated as quantity multiplied by unit_price



9.5.5 Table and Cross-Table Rules
These types of rules assert that the value associated with one data element is consistent conditioned on values of other data elements within the same table or in other tables. Some examples are shown in Table 9.4.



Table 9.4 Examples of Table or Cross-Table Rules

Dimension

Accuracy	
A data element's value is accurate when compared to a system of record when the value is dependent on other data element values for system of record lookup (including other tables)	
The telephone_number for this office is equal to the telephone_number in the directory for this office_identifier

Consistency	

One data element's value is consistent with other data elements' values (including other tables)	
The household_income value is within 10% plus or minus the median_income value for homes within this zip_code

Completeness	

When other data element values observe a defined condition, a data element's value is not null (including other tables)	
Look up the customer's profile, and if customer_status is “preferred” then discount may not be null

Reasonableness	

When other data element values observe a defined condition, a data element's value must conform to reasonable expectations (including other tables)	
Today's closing_price should not be 2% more or less than the running average of closing prices for the past 30 days

Reasonableness	

The value of a data attribute in one data instance must be reasonable in relation to other data instances in the same set	
Flag any values of the duration attribute that are more than 2 times the standard deviation of all the duration attribute values
Currency	When other data element values observe a defined condition, a data element's value has been refreshed 
within the specified time period (including other tables)	
Look up the product code in the supplier catalog, and if the price has been updated within the past 24 hours then product_price must be updated


Data Harmonization

1. Preliminary


1. Perform the initial activities to collect the information needed to perform the harmonization:
1.1. Select the data sets to be harmonized
1.2. Obtain documentations for selected data sets (may include data models, data dictionaries, definitions, forms and related guidance, and any other reference information)
1.3. Analyze reference source documents – review documents, identify critical data elements and important data element metadata
1.4. Enter source data information into data harmonization template

# reference data management tables
2. Extract or collect the metadata from the reference sources. Information may be embedded in data dictionaries, text memos, or even in instructions for filling out forms. For each data element, complete the data harmonization template as follows:


2.1. ID – assign a unique identifier to the data element
2.2. Data element name – provide a contextually meaningful unique name for the data element
2.3. Standard name – not done at this point (but a name that conforms to a naming standard will be assigned at a later stage)
2.4. Definition – document a definition extracted from an authoritative source document
2.5. SOR(Authoritative source) – identify reference source for the data element definition; note if there is more than one source, and which source carries the highest priority
2.6. Length – the physical length of the data element
2.7. Type – the data type of the data element
2.8. Business rules – document any constraints on the value set, such as enumerated data domains, format specifications, or value ranges
2.9. Issues/comments – capture observations, assumption, suggestions, questions associated with data element
2.10. Source/application inventory – enumerate the data sets/applications/forms/processes from which the metadata for this data element has been collected; this will be used to assess overlap, similarity, and identical data attributes across multiple data sets
2.11. Validated by – filled by data analyst assigned for validation
2.12. Validation date – filled by data analyst assigned for validation


3. Validate and identify anomalies



3. Verify that data element metadata has been accurately and consistently captured:
3.1. The name must match the original source
3.2. The data element is assigned a unique identifier
3.3. All relevant definitions and business rules appear in the extracted list of data elements
3.4. Definitions do not use the data element name as part of definition
3.5. Definitions are not instructions for providing a value
3.6. Business rules must be specific to data element
3.7. Business rules qualify the value set for the data element
3.8. Note any ambiguous definitions (do not describe the business concept that is stored in the data element, circular definitions, etc.)
3.9. Inconsistent definitions – definitions from two applications/sources that disagree
3.10. Business rules as definitions – definitions that are actually rules constraining data value set
3.11. Inconsistent data types – values are seen in more than one data type (e.g., alphanumeric vs. numeric)
3.12. Inconsistent formats – data element is seen in conflicting formats
3.13. Abbreviations – abbreviations used instead of full names


4. Collate and integrate


4. Determine if data elements are similar or identical:
4.1. Determine identical data elements – data elements that have the same definition, authoritative source, business rules, length, type, and value set can be considered to be identical
4.2. Provide standardized name – assign each instance of a set of identical data elements with a standard name, and note the relationship between the source data elements; each data element will retain its unique ID
4.3. Establish data element similarity – data elements are considered similar if the elements share the same definition and authoritative source but disagree about business rules, length, or type
4.4. Document comments – for each resolved set of data elements, mark the resolution in the issues/comments column
4.5. Resolve similarity – for data elements marked as similar, research to determine whether the data elements are identical


The Data Harmonization Template

ID	Data Element Name	Standard Name	Label	Definition	Authoritative Source	Length	Type	Business Rules	Issues/ Comments	Mapping	Validated/ Validated By/ Validation Date


********
Data profileing and analysis

column analysis
table analysis
Cross-table analysis


Column Analysis
================
1. Document number of distinct values.
a. Note any duplicate values when the column should be unique.
b. Note whether the number of distinct values is consistent with expectations.
2. Evaluate the highest (maximum) values.
a. Note whether there are values that are greater than expected.
b. Note comparisons with defined or expected maximum values.
3. Evaluate the lowest (minimum) values.
a. Note whether there are values that are lower than expected.
b. Note comparisons with defined or expected minimum values.
4. Document the mean and median value (for numeric data).
5. Document the standard deviation (for numeric data).
a. If there is a small standard deviation, examin 
   outliers distant from median, and document potential irregularities.
b. Examine values that are more than three standard deviations from the mean, and document potential irregularities.
6. Note the number of nulls.
a. Note the existence of nulls if the value should never be null.
b. Note comparison between null and value population.
7. Discovered patterns, if any.
a. Document potential irregularities with data values matching infrequent patterns.
8. Evaluate potential overloaded use.
a. Note when values recognizable as belonging to multiple data domains appear in one column.
9. For each column’s set of values:
a. Verify inferred data type is consistent with document data type.
b. Verify validity of values.
10. Evaluate the most frequently occurring values.
a. Note unexpectedly large number of null values.
b. Note if frequently appearing values represent default values.
c. Note if frequently appearing values are not valid values.
11. Evaluate the least frequently occurring values.
a. Note any unexpected or invalid values.
12. Perform a visual inspection.
a. Sort by value, scan for consecutive values, and document findings.
b. Sort by value, scan for similar values, and document findings.


11.4.3 Cross-Table Analysis
Cross-table analysis looks at the relational structure inherent in a collection of data tables, such as:

• Assess referential integrity: identify any orphaned records in the related table;
• Validate reference data: confirm that data elements that are expected to draw their values from existing code table comply with the expectations; and
• Seek cross-table (join) duplicates: identify overlapping records across multiple tables where only one record should exist.


Rough outline of a Data Quality Assessment Report contains the following sections:

1. Executive summary, providing high-level overview of the task and the results
2. Introduction, describing how data profiling and additional analyses were used to assess the quality of selected data sets
3. Goals, enumerating the specific goals of the analysis, such as “reviewing the quality of data prior to integration in a data warehouse”
4. Scope, detailing the results of the scope assessment and the identified business impacts
5. Approach, describing the details of how the assessment was performed, namely, profiling and other analyses performed, the identified critical data elements, proposed measurements, and the techniques applied
6. Data analysis results, providing the listed observations of potential anomalies and reasons for consideration
7. Recommendations, detailing the suggestions resulting from the synthesis
8. Open issues, listing any unresolved questions
9. Next steps, providing the action items resulting from the recommendations review and any requirements to resolve any of the open issues.
10. Additional supporting material, including such information as raw statistics from the column, table, and cross-table templates and any other (nonprofiling) analyses to support the recommendations


Parsing
The goal of parsing is to scan the value(s) that are stored in a data element to identify those that map to which defined token types. In order to do this, we need these pieces:

1. Token types, definitions, and the types of the tokens expected to be found in the data attributes (for person names, this might include title, first_name, last_name, generational_indicator, and suffix)
2. Value domains, or the set of valid values for each data component type (such as enumerated lists of valid first names or valid titles)
3. Formats and patterns for collections of tokens that must be recognized together
4. Acceptable forms that the data may take (such as “mixed case,” or “without punctuation”)
5. A parser, which is the tool used to recognize when data values match defined patterns
6. Methods for arbitrating situations where character strings map to more than one token or pattern
7. Error handling, or a means for tagging records that have unidentified data components


16.5 Similarity Analysis and the Matching Process
16.6 Matching Algorithms
These measures often look at perceived similarity between the character strings and rely on different approximate matching techniques, such as the following:

• Parsing and standardization
• Abbreviation expansion
• Edit distance
• Phonetic comparison
• N-gramming


Registry model
****************888

19.7.1 Registry
In a registry style master data architecture, a thin index captures the minimal set of identifying attribute (see Figure 19.2) values. This index, in which each unique entity is “registered,” maps the identifying attributes to records within the source data systems that contain information about the uniquely identified entity. Since most of the entity's data attributes are stored in the original source systems, the registry is used as a means of access to master data, which involves:

• Using an identity resolution process to search through the master registry index to find candidate matches and evaluate similarity of candidates to the sought-after record to determine a close match;
• Given a set of matches, following the pointers from the registry entries for that entity, then accessing and retrieving records for the found entity from the application data systems; and
• Returning the master data, either by provisioning the delivery of all accessed record, or optionally applying a set of survivorship rules to materialize a single consolidated master record.
 image
Figure 19.2 A registry maintains identifying information and maps to existing data assets.

The following are some of the assumptions regarding the registry style:

• Data attributes in the master index must be linked to existing application data silos to support existing processing streams.
• All input records are cleansed and enhanced before persistence.
• The registry contains only attributes that contain the identifying information required to disambiguate entities.
• There is a unique instance within the registry for each master entity, and only identifying information is maintained within the master record.
• The persistent copies for all attribute values are maintained in (one or more) application data systems.
• Survivorship rules on consolidation are applied at time of data access, not time of data registration.




******************** Template for TAG for tools question*******************
The information production flow notes how information flows through these stages within an application:

1. Data sources that are used by the business process
2. Processing stage, noting any processing performed on the data
3. Storage locations, listing the data elements that are stored and the system where the data is stored
4. Validation points, where there are checks for data quality criteria, and for each location, the list of data quality validations performed at that point
5. Decision points at which the processing stream is directed based on the result of evaluating different conditions
Any data handoffs between processing stages, application boundaries, or system boundaries are also noted.
********************

